graph LR
    A[Reinforcement Learning] --> B(Policy Optimization);
    B --> C[Policy Gradient Methods];
    C --> D{Instability (Naive PG)};
    C --> E[Trust Region Policy Optimization (TRPO)];
    C --> F[Proximal Policy Optimization (PPO)];
    C --> G[Simple Policy Optimization (SPO)];

    E -- Robustness --> H[Constrained Optimization];
    E -- Robustness --> I[Second-order Optimization];
    E -- Complexity --> J[Computationally Intensive];
    E -- Monotonic Improvement --> B;

    F -- Efficiency --> K[First-order Optimization];
    F -- Simplicity --> L[Clipped Surrogate Objective];
    F -- Practicality --> M[Widely Used];
    F -- Less Robustness --> D;
    F -- Simplifies TRPO --> E;
    F -- Stable Learning --> B;

    G -- Balances Robustness & Efficiency --> N[Modified PPO Loss Function];
    G -- Improved Stability --> D;
    G -- First-order Optimization --> K;
    G -- Potentially Superior Performance --> F & E;
    G -- Aims for TRPO Robustness --> E;
    G -- Retains PPO Efficiency --> F;
    G -- Refines PPO --> F;
    G -- Unconstrained Optimization --> K;

    style D fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#aaf,stroke:#333,stroke-width:2px
    style E fill:#faa,stroke:#333,stroke-width:2px