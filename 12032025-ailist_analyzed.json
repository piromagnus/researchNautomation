[
    {
        "title": "Pok\u00e9Champ",
        "not_found": true,
        "processed_at": "2025-03-12 14:04:54+0000"
    },
    {
        "title": "(How) Do Language Models Track State?",
        "abstract": "Transformer language models (LMs) exhibit behaviors -- from storytelling to code generation -- that appear to require tracking the unobserved state of an evolving world. How do they do so? We study state tracking in LMs trained or fine-tuned to compose permutations (i.e., to compute the order of a set of objects after a sequence of swaps). Despite the simple algebraic structure of this problem, many other tasks (e.g., simulation of finite automata and evaluation of boolean expressions) can be reduced to permutation composition, making it a natural model for state tracking in general. We show that LMs consistently learn one of two state tracking mechanisms for this task. The first closely resembles the \"associative scan\" construction used in recent theoretical work by Liu et al. (2023) and Merrill et al. (2024). The second uses an easy-to-compute feature (permutation parity) to partially prune the space of outputs, then refines this with an associative scan. The two mechanisms exhibit markedly different robustness properties, and we show how to steer LMs toward one or the other with intermediate training tasks that encourage or suppress the heuristics. Our results demonstrate that transformer LMs, whether pretrained or fine-tuned, can learn to implement efficient and interpretable state tracking mechanisms, and the emergence of these mechanisms can be predicted and controlled.\n21 pages, 17 figures, 1 table. Code:\n  http://github.com/belindal/state-tracking",
        "authors": [
            "Belinda Z. Li",
            "Zifan Carl Guo",
            "Jacob Andreas"
        ],
        "arxiv_categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "arxiv_primary_category": "cs.CL",
        "date": "2025-03-11 15:36:40+0000",
        "link": "http://arxiv.org/abs/2503.02854v2",
        "pdf_url": "http://arxiv.org/pdf/2503.02854v2",
        "arxiv_id": "2503.02854v2",
        "processed_at": "2025-03-12 14:04:50+0000",
        "not_found": false,
        "repo": "http://github.com/belindal/state-tracking",
        "stars": 0,
        "main_task": "State Tracking in Language Models",
        "contributions": "- Identified two distinct state-tracking mechanisms in transformers: associative scan and permutation parity refinement.\n- Demonstrated differential robustness of these mechanisms for state tracking tasks.\n- Showed how intermediate training tasks can steer LMs toward specific state tracking mechanisms.",
        "summary": "The paper identifies two distinct mechanisms that transformer language models use for state tracking through permutation composition: an associative scan mechanism and a combination of permutation parity with associative scanning. The study shows how intermediate training tasks can steer LMs towards adopting one mechanism over the other, highlighting their different robustness properties. This demonstrates that efficient and interpretable state tracking in LMs can be predicted and controlled.",
        "analyzed_at": "2025-03-12 14:06:16+0000"
    },
    {
        "title": "Muon is Scalable for LLM Training",
        "abstract": "Recently, the Muon optimizer based on matrix orthogonalization has demonstrated strong results in training small-scale language models, but the scalability to larger models has not been proven. We identify two crucial techniques for scaling up Muon: (1) adding weight decay and (2) carefully adjusting the per-parameter update scale. These techniques allow Muon to work out-of-the-box on large-scale training without the need of hyper-parameter tuning. Scaling law experiments indicate that Muon achieves $\\sim\\!2\\times$ computational efficiency compared to AdamW with compute optimal training.   Based on these improvements, we introduce Moonlight, a 3B/16B-parameter Mixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model improves the current Pareto frontier, achieving better performance with much fewer training FLOPs compared to prior models.   We open-source our distributed Muon implementation that is memory optimal and communication efficient. We also release the pretrained, instruction-tuned, and intermediate checkpoints to support future research.",
        "authors": [
            "Jingyuan Liu",
            "Jianlin Su",
            "Xingcheng Yao",
            "Zhejun Jiang",
            "Guokun Lai",
            "Yulun Du",
            "Yidao Qin",
            "Weixin Xu",
            "Enzhe Lu",
            "Junjie Yan",
            "Yanru Chen",
            "Huabin Zheng",
            "Yibo Liu",
            "Shaowei Liu",
            "Bohong Yin",
            "Weiran He",
            "Han Zhu",
            "Yuzhi Wang",
            "Jianzhou Wang",
            "Mengnan Dong",
            "Zheng Zhang",
            "Yongsheng Kang",
            "Hao Zhang",
            "Xinran Xu",
            "Yutao Zhang",
            "Yuxin Wu",
            "Xinyu Zhou",
            "Zhilin Yang"
        ],
        "arxiv_categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "arxiv_primary_category": "cs.LG",
        "date": "2025-02-24 09:12:29+0000",
        "link": "http://arxiv.org/abs/2502.16982v1",
        "pdf_url": "http://arxiv.org/pdf/2502.16982v1",
        "arxiv_id": "2502.16982v1",
        "processed_at": "2025-03-12 14:04:50+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Task type: LLM Training Optimization",
        "contributions": "- Introduction of weight decay and per-parameter update adjustments to scale Muon optimizer.\n- Achievement of ~2\u00d7 computational efficiency over AdamW with compute optimal training.\n- Development and release of Moonlight model, along with an open-source distributed Muon implementation.",
        "summary": "",
        "analyzed_at": "2025-03-12 14:06:14+0000"
    },
    {
        "title": "The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models",
        "abstract": "Improving the reasoning capabilities of large language models (LLMs) typically requires supervised fine-tuning with labeled data or computationally expensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which leverages the observation of Prefix Self-Consistency -- the shared initial reasoning steps across diverse solution trajectories -- to enhance LLM reasoning efficiency. By training exclusively on the initial prefix substrings (as few as 8 tokens), UPFT removes the need for labeled data or exhaustive sampling. Experiments on reasoning benchmarks show that UPFT matches the performance of supervised methods such as Rejection Sampling Fine-Tuning, while reducing training time by 75% and sampling cost by 99%. Further analysis reveals that errors tend to appear in later stages of the reasoning process and that prefix-based training preserves the model's structural knowledge. This work demonstrates how minimal unsupervised fine-tuning can unlock substantial reasoning gains in LLMs, offering a scalable and resource-efficient alternative to conventional approaches.",
        "authors": [
            "Ke Ji",
            "Jiahao Xu",
            "Tian Liang",
            "Qiuzhi Liu",
            "Zhiwei He",
            "Xingyu Chen",
            "Xiaoyuan Liu",
            "Zhijie Wang",
            "Junying Chen",
            "Benyou Wang",
            "Zhaopeng Tu",
            "Haitao Mi",
            "Dong Yu"
        ],
        "arxiv_categories": [
            "cs.CL"
        ],
        "arxiv_primary_category": "cs.CL",
        "date": "2025-03-04 18:56:03+0000",
        "link": "http://arxiv.org/abs/2503.02875v1",
        "pdf_url": "http://arxiv.org/pdf/2503.02875v1",
        "arxiv_id": "2503.02875v1",
        "processed_at": "2025-03-12 14:04:51+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Unsupervised Prefix Fine-Tuning for Language Models",
        "contributions": "- Introduces Unsupervised Prefix Fine-Tuning (UPFT) for LLMs.\n- Trains on initial token prefixes, reducing need for labeled data.\n- Matches supervised methods\u2019 performance while drastically cutting costs.",
        "summary": "The paper introduces Unsupervised Prefix Fine-Tuning (UPFT), a technique that enhances LLMs' reasoning by focusing on initial prefix substrings of reasoning steps. This methodology significantly reduces the need for labeled data or exhaustive sampling. Experiments demonstrate UPFT achieves performance similar to supervised methods while cutting training time by 75% and reducing sampling costs by 99%.",
        "analyzed_at": "2025-03-12 14:06:15+0000"
    },
    {
        "title": "Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining",
        "abstract": "The impressive capabilities of Large Language Models (LLMs) across diverse tasks are now well-established, yet their effective deployment necessitates careful hyperparameter optimization. Through extensive empirical studies involving grid searches across diverse configurations, we discover universal scaling laws governing these hyperparameters: optimal learning rate follows a power-law relationship with both model parameters and data sizes, while optimal batch size scales primarily with data sizes. Our analysis reveals a convex optimization landscape for hyperparameters under fixed models and data size conditions. This convexity implies an optimal hyperparameter plateau. We contribute a universal, plug-and-play optimal hyperparameter tool for the community. Its estimated values on the test set are merely 0.09% away from the globally optimal LLM performance found via an exhaustive search. These laws demonstrate remarkable robustness across variations in model sparsity, training data distribution, and model shape. To our best known, this is the first work that unifies different model shapes and structures, such as Mixture-of-Experts models and dense transformers, as well as establishes optimal hyperparameter scaling laws across diverse data distributions. This exhaustive optimization process demands substantial computational resources, utilizing nearly one million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and hyperparameters from scratch and consuming approximately 100 trillion tokens in total. To facilitate reproducibility and further research, we will progressively release all loss measurements and model checkpoints through our designated repository https://step-law.github.io/\n19 pages",
        "authors": [
            "Houyi Li",
            "Wenzheng Zheng",
            "Jingcheng Hu",
            "Qiufeng Wang",
            "Hanshan Zhang",
            "Zili Wang",
            "Shijie Xuyang",
            "Yuantao Fan",
            "Shuigeng Zhou",
            "Xiangyu Zhang",
            "Daxin Jiang"
        ],
        "arxiv_categories": [
            "cs.LG",
            "cs.AI",
            "F.2.2; I.2.7"
        ],
        "arxiv_primary_category": "cs.LG",
        "date": "2025-03-09 17:59:40+0000",
        "link": "http://arxiv.org/abs/2503.04715v2",
        "pdf_url": "http://arxiv.org/pdf/2503.04715v2",
        "arxiv_id": "2503.04715v2",
        "processed_at": "2025-03-12 14:05:06+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Hyperparameter Optimization",
        "contributions": "- **Optimal Learning Rate Scaling Law**: A power-law relationship between learning rate, model parameters, and data sizes.\n  \n- **Batch Size Scaling Discovery**: Optimal batch size primarily scales with data sizes, revealing a universal pattern.\n  \n- **Convex Optimization Landscape**: Demonstrates optimal hyperparameter plateau under fixed models/data conditions.",
        "summary": "The research paper introduces a universal hyperparameter scaling law for large language models (LLMs) that optimizes learning rates and batch sizes. Through extensive empirical grid searches, it identifies a power-law relationship between these hyperparameters and model/data size. The study reveals a convex optimization landscape implying an optimal plateau for fixed conditions. This results in a near-optimal hyperparameter tool with minimal deviation from global performance. These findings are robust across different model architectures and data distributions, marking a unified approach to LLM training efficiency.",
        "analyzed_at": "2025-03-12 14:06:14+0000"
    },
    {
        "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
        "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.\n39 pages",
        "authors": [
            "Microsoft",
            ":",
            "Abdelrahman Abouelenin",
            "Atabak Ashfaq",
            "Adam Atkinson",
            "Hany Awadalla",
            "Nguyen Bach",
            "Jianmin Bao",
            "Alon Benhaim",
            "Martin Cai",
            "Vishrav Chaudhary",
            "Congcong Chen",
            "Dong Chen",
            "Dongdong Chen",
            "Junkun Chen",
            "Weizhu Chen",
            "Yen-Chun Chen",
            "Yi-ling Chen",
            "Qi Dai",
            "Xiyang Dai",
            "Ruchao Fan",
            "Mei Gao",
            "Min Gao",
            "Amit Garg",
            "Abhishek Goswami",
            "Junheng Hao",
            "Amr Hendy",
            "Yuxuan Hu",
            "Xin Jin",
            "Mahmoud Khademi",
            "Dongwoo Kim",
            "Young Jin Kim",
            "Gina Lee",
            "Jinyu Li",
            "Yunsheng Li",
            "Chen Liang",
            "Xihui Lin",
            "Zeqi Lin",
            "Mengchen Liu",
            "Yang Liu",
            "Gilsinia Lopez",
            "Chong Luo",
            "Piyush Madan",
            "Vadim Mazalov",
            "Arindam Mitra",
            "Ali Mousavi",
            "Anh Nguyen",
            "Jing Pan",
            "Daniel Perez-Becker",
            "Jacob Platin",
            "Thomas Portet",
            "Kai Qiu",
            "Bo Ren",
            "Liliang Ren",
            "Sambuddha Roy",
            "Ning Shang",
            "Yelong Shen",
            "Saksham Singhal",
            "Subhojit Som",
            "Xia Song",
            "Tetyana Sych",
            "Praneetha Vaddamanu",
            "Shuohang Wang",
            "Yiming Wang",
            "Zhenghao Wang",
            "Haibin Wu",
            "Haoran Xu",
            "Weijian Xu",
            "Yifan Yang",
            "Ziyi Yang",
            "Donghan Yu",
            "Ishmam Zabir",
            "Jianwen Zhang",
            "Li Lyna Zhang",
            "Yunan Zhang",
            "Xiren Zhou"
        ],
        "arxiv_categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "arxiv_primary_category": "cs.CL",
        "date": "2025-03-07 09:05:58+0000",
        "link": "http://arxiv.org/abs/2503.01743v2",
        "pdf_url": "http://arxiv.org/pdf/2503.01743v2",
        "arxiv_id": "2503.01743v2",
        "processed_at": "2025-03-12 14:05:09+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Multimodal Language Models Integration",
        "contributions": "- **Compact Model with Superior Performance:** Phi-4-Mini achieves high performance in complex tasks with fewer parameters than comparable models and matches twice its size on specific tasks.\n\n- **Multimodal Integration via LoRA Adapters:** Phi-4-Multimodal effectively integrates text, vision, and speech inputs using a novel modality extension approach that leverages LoRA adapters for efficient multimodal processing.\n\n- **Enhanced Reasoning with Synthetic Data:** The use of a carefully curated synthetic data recipe in Phi-4-Mini significantly improves reasoning capabilities, enabling it to perform on par or better than larger models.",
        "summary": "The paper introduces Phi-4-Mini and Phi-4-Multimodal models that leverage LoRA adapters for efficiency. Phi-4-Mini uses a 3.8 billion parameter setup with enhanced vocabulary, outperforming similar-sized language models in complex reasoning. Phi-4-Multimodal combines text, vision, and speech/audio inputs using modality-specific routers, achieving top performance in OpenASR tasks and excelling across multimodal scenarios. These innovations result in compact yet highly capable models.",
        "analyzed_at": "2025-03-12 14:06:37+0000"
    },
    {
        "title": "L1: Controlling How Long A Reasoning Model Thinks With RL",
        "not_found": true,
        "processed_at": "2025-03-12 14:05:13+0000"
    },
    {
        "title": "When Can You Get Away with Low Memory Adam?",
        "abstract": "Adam is the go-to optimizer for training modern machine learning models, but it requires additional memory to maintain the moving averages of the gradients and their squares. While various low-memory optimizers have been proposed that sometimes match the performance of Adam, their lack of reliability has left Adam as the default choice. In this work, we apply a simple layer-wise Signal-to-Noise Ratio (SNR) analysis to quantify when second-moment tensors can be effectively replaced by their means across different dimensions. Our SNR analysis reveals how architecture, training hyperparameters, and dataset properties impact compressibility along Adam's trajectory, naturally leading to $\\textit{SlimAdam}$, a memory-efficient Adam variant. $\\textit{SlimAdam}$ compresses the second moments along dimensions with high SNR when feasible, and leaves when compression would be detrimental. Through experiments across a diverse set of architectures and training scenarios, we show that $\\textit{SlimAdam}$ matches Adam's performance and stability while saving up to $98\\%$ of total second moments. Code for $\\textit{SlimAdam}$ is available at https://github.com/dayal-kalra/low-memory-adam.\nAcknowledgement updates and minor writing edits",
        "authors": [
            "Dayal Singh Kalra",
            "John Kirchenbauer",
            "Maissam Barkeshli",
            "Tom Goldstein"
        ],
        "arxiv_categories": [
            "cs.LG",
            "cond-mat.dis-nn",
            "stat.ML"
        ],
        "arxiv_primary_category": "cs.LG",
        "date": "2025-03-06 18:38:33+0000",
        "link": "http://arxiv.org/abs/2503.01843v2",
        "pdf_url": "http://arxiv.org/pdf/2503.01843v2",
        "arxiv_id": "2503.01843v2",
        "processed_at": "2025-03-12 14:05:09+0000",
        "not_found": false,
        "repo": "https://github.com/dayal-kalra/low-memory-adam",
        "stars": 0,
        "main_task": "Research Category: Optimizer Memory Efficiency\n\nNote: While it involves machine learning, this task is specifically about optimizing the memory efficiency of an optimizer algorithm.",
        "contributions": "- Introduced SNR analysis to quantify when second-moment tensors can be effectively compressed.\n- Proposed $\\textit{SlimAdam}$, compressing second moments based on high SNR dimensions while maintaining performance and stability.\n- Demonstrated that $\\textit{SilamAdam}$ achieves up to 98% memory savings across diverse architectures without compromising Adam's performance.",
        "summary": "",
        "analyzed_at": "2025-03-12 14:06:30+0000"
    },
    {
        "title": "Software Model Evolution with Large Language Models: Experiments on Simulated, Public, and Industrial Datasets",
        "abstract": "Modeling structure and behavior of software systems plays a crucial role in the industrial practice of software engineering. As with other software engineering artifacts, software models are subject to evolution. Supporting modelers in evolving software models with recommendations for model completions is still an open problem, though. In this paper, we explore the potential of large language models for this task. In particular, we propose an approach, RAMC, leveraging large language models, model histories, and retrieval-augmented generation for model completion. Through experiments on three datasets, including an industrial application, one public open-source community dataset, and one controlled collection of simulated model repositories, we evaluate the potential of large language models for model completion with RAMC. We found that large language models are indeed a promising technology for supporting software model evolution (62.30% semantically correct completions on real-world industrial data and up to 86.19% type-correct completions). The general inference capabilities of large language models are particularly useful when dealing with concepts for which there are few, noisy, or no examples at all.",
        "authors": [
            "Christof Tinnes",
            "Alisa Welter",
            "Sven Apel"
        ],
        "arxiv_categories": [
            "cs.SE",
            "cs.AI",
            "94-04",
            "D.2.2"
        ],
        "arxiv_primary_category": "cs.SE",
        "date": "2024-12-10 09:54:43+0000",
        "link": "http://arxiv.org/abs/2406.17651v5",
        "pdf_url": "http://arxiv.org/pdf/2406.17651v5",
        "arxiv_id": "2406.17651v5",
        "processed_at": "2025-03-12 14:05:22+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Software Model Completion",
        "contributions": "- Proposed RAMC approach using LLMs for model completion.\n- Leveraged retrieval-augmented generation and model histories in RAMC.\n- Demonstrated high accuracy (up to 86.19%) on diverse datasets.",
        "summary": "The paper introduces RAMC, a method leveraging large language models (LLMs) for software model completion using retrieval-augmented generation and model histories. The methodology involves testing on simulated, public, and industrial datasets to evaluate LLMs' effectiveness in completing software models. Results demonstrate significant potential: 62.30% semantically correct completions on industrial data and up to 86.19% type-correct completions, highlighting the utility of LLMs even with sparse examples.",
        "analyzed_at": "2025-03-12 14:06:29+0000"
    },
    {
        "title": "Deep Learning is Not So Mysterious or Different",
        "abstract": "Deep neural networks are often seen as different from other model classes by defying conventional notions of generalization. Popular examples of anomalous generalization behaviour include benign overfitting, double descent, and the success of overparametrization. We argue that these phenomena are not distinct to neural networks, or particularly mysterious. Moreover, this generalization behaviour can be intuitively understood, and rigorously characterized using long-standing generalization frameworks such as PAC-Bayes and countable hypothesis bounds. We present soft inductive biases as a key unifying principle in explaining these phenomena: rather than restricting the hypothesis space to avoid overfitting, embrace a flexible hypothesis space, with a soft preference for simpler solutions that are consistent with the data. This principle can be encoded in many model classes, and thus deep learning is not as mysterious or different from other model classes as it might seem. However, we also highlight how deep learning is relatively distinct in other ways, such as its ability for representation learning, phenomena such as mode connectivity, and its relative universality.",
        "authors": [
            "Andrew Gordon Wilson"
        ],
        "arxiv_categories": [
            "cs.LG",
            "stat.ML"
        ],
        "arxiv_primary_category": "cs.LG",
        "date": "2025-03-03 22:56:04+0000",
        "link": "http://arxiv.org/abs/2503.02113v1",
        "pdf_url": "http://arxiv.org/pdf/2503.02113v1",
        "arxiv_id": "2503.02113v1",
        "processed_at": "2025-03-12 14:05:09+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Theoretical analysis of deep learning generalization.",
        "contributions": "- The phenomena of benign overfitting, double descent, and success of overparametrization are not unique to neural networks and can be understood using frameworks like PAC-Bayes and countable hypothesis bounds.\n- Soft inductive biases serve as a unifying principle, explaining generalization behavior across various model classes by preferring simpler solutions consistent with the data.\n- Deep learning's distinct features include its ability for representation learning, phenomena such as mode connectivity, and relative universality.",
        "summary": "The paper posits that the generalization behavior of deep neural networks, such as benign overfitting and double descent, are not exclusive or mysterious phenomena; they can be explained using established frameworks like PAC-Bayes. The key innovation is the concept of \"soft inductive biases,\" which encourage a flexible hypothesis space with a preference for simpler solutions that align with data. These principles apply across various model classes, suggesting deep learning is less unique than often perceived. However, it also notes distinct aspects of deep learning such as representation learning and mode connectivity.",
        "analyzed_at": "2025-03-12 14:06:32+0000"
    },
    {
        "title": "All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning",
        "abstract": "From a first-principles perspective, it may seem odd that the strongest results in foundation model fine-tuning (FT) are achieved via a relatively complex, two-stage training procedure. Specifically, one first trains a reward model (RM) on some dataset (e.g. human preferences) before using it to provide online feedback as part of a downstream reinforcement learning (RL) procedure, rather than directly optimizing the policy parameters on the dataset via offline maximum likelihood estimation. In fact, from an information-theoretic perspective, we can only lose information via passing through a reward model and cannot create any new information via on-policy sampling. To explain this discrepancy, we scrutinize several hypotheses on the value of RL in FT through both theoretical and empirical lenses. Of the hypotheses considered, we find the most support for the explanation that on problems with a generation-verification gap, the combination of the ease of learning the relatively simple RM (verifier) from the preference data, coupled with the ability of the downstream RL procedure to then filter its search space to the subset of policies (generators) that are optimal for relatively simple verifiers is what leads to the superior performance of online FT.",
        "authors": [
            "Gokul Swamy",
            "Sanjiban Choudhury",
            "Wen Sun",
            "Zhiwei Steven Wu",
            "J. Andrew Bagnell"
        ],
        "arxiv_categories": [
            "cs.LG"
        ],
        "arxiv_primary_category": "cs.LG",
        "date": "2025-03-03 00:15:19+0000",
        "link": "http://arxiv.org/abs/2503.01067v1",
        "pdf_url": "http://arxiv.org/pdf/2503.01067v1",
        "arxiv_id": "2503.01067v1",
        "processed_at": "2025-03-12 14:05:25+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Reinforcement Learning",
        "contributions": "- **Exploration of Two-Stage Training Efficiency**: Investigates why a complex two-stage procedure (reward model + RL) yields superior fine-tuning results compared to direct offline optimization.\n  \n- **Analysis of Information-Theoretic Perspectives**: Examines information loss when using reward models in the context of reinforcement learning-based fine-tuning.\n\n- **Identification of Generation-Verification Gap**: Demonstrates how a generation-verification gap and simple verifier learning contribute to RL's effectiveness in filtering optimal policy search spaces.",
        "summary": "",
        "analyzed_at": "2025-03-12 14:06:54+0000"
    },
    {
        "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs",
        "abstract": "Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.",
        "authors": [
            "Kanishk Gandhi",
            "Ayush Chakravarthy",
            "Anikait Singh",
            "Nathan Lile",
            "Noah D. Goodman"
        ],
        "arxiv_categories": [
            "cs.CL",
            "cs.LG"
        ],
        "arxiv_primary_category": "cs.CL",
        "date": "2025-03-03 08:46:22+0000",
        "link": "http://arxiv.org/abs/2503.01307v1",
        "pdf_url": "http://arxiv.org/pdf/2503.01307v1",
        "arxiv_id": "2503.01307v1",
        "processed_at": "2025-03-12 14:05:40+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Reinforcement Learning",
        "contributions": "- Introduced framework for analyzing four cognitive behaviors: verification, backtracking, subgoal setting, and backward chaining in language models.\n- Demonstrated that priming with reasoning behavior patterns improves Llama's RL performance to match Qwen.\n- Showed continued pretraining on filtered OpenWebMath data enhances Llama\u2019s self-improvement trajectory.",
        "summary": "",
        "analyzed_at": "2025-03-12 14:06:52+0000"
    },
    {
        "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
        "abstract": "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by $24.3\\%$ over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by $21.9$ on COCO's two-shot setting and $15.4$ on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.\nproject page: https://github.com/Liuziyu77/Visual-RFT",
        "authors": [
            "Ziyu Liu",
            "Zeyi Sun",
            "Yuhang Zang",
            "Xiaoyi Dong",
            "Yuhang Cao",
            "Haodong Duan",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "arxiv_categories": [
            "cs.CV"
        ],
        "arxiv_primary_category": "cs.CV",
        "date": "2025-03-03 18:16:32+0000",
        "link": "http://arxiv.org/abs/2503.01785v1",
        "pdf_url": "http://arxiv.org/pdf/2503.01785v1",
        "arxiv_id": "2503.01785v1",
        "processed_at": "2025-03-12 14:05:25+0000",
        "not_found": false,
        "repo": "https://github.com/Liuziyu77/Visual-RFT",
        "stars": 0,
        "main_task": "Visual Reinforcement Fine-Tuning",
        "contributions": "- Introduced Visual Reinforcement Fine-Tuning (Visual-RFT).\n- Designed verifiable reward functions for different perception tasks.\n- Applied Group Relative Policy Optimization (GRPO) with visual rewards to improve model performance.",
        "summary": "Visual-RFT extends Reinforcement Fine-Tuning to visual tasks, using Large Vision-Language Models and verifiable reward functions via Group Relative Policy Optimization for updating models. It outperforms Supervised Fine-tuning in various benchmarks, achieving significant accuracy improvements in few-shot learning scenarios.",
        "analyzed_at": "2025-03-12 14:06:56+0000"
    },
    {
        "title": "Experience Replay with Random Reshuffling",
        "abstract": "Experience replay is a key component in reinforcement learning for stabilizing learning and improving sample efficiency. Its typical implementation samples transitions with replacement from a replay buffer. In contrast, in supervised learning with a fixed dataset, it is a common practice to shuffle the dataset every epoch and consume data sequentially, which is called random reshuffling (RR). RR enjoys theoretically better convergence properties and has been shown to outperform with-replacement sampling empirically. To leverage the benefits of RR in reinforcement learning, we propose sampling methods that extend RR to experience replay, both in uniform and prioritized settings. We evaluate our sampling methods on Atari benchmarks, demonstrating their effectiveness in deep reinforcement learning.",
        "authors": [
            "Yasuhiro Fujita"
        ],
        "arxiv_categories": [
            "cs.LG",
            "cs.AI"
        ],
        "arxiv_primary_category": "cs.LG",
        "date": "2025-03-04 04:37:22+0000",
        "link": "http://arxiv.org/abs/2503.02269v1",
        "pdf_url": "http://arxiv.org/pdf/2503.02269v1",
        "arxiv_id": "2503.02269v1",
        "processed_at": "2025-03-12 14:05:25+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Reinforcement Learning",
        "contributions": "- Proposed new RR-based sampling methods for experience replay.\n- Extended RR to both uniform and prioritized experience replay settings.\n- Demonstrated effectiveness through empirical evaluation on Atari benchmarks.",
        "summary": "Experience Replay with Random Reshuffling improves stability and efficiency in RL by applying a shuffling technique from supervised learning. The methodology involves extending RR sampling methods to both uniform and prioritized replay buffers in RL, addressing theoretical convergence benefits. Results show enhanced performance on Atari benchmarks compared to traditional experience replay methods.",
        "analyzed_at": "2025-03-12 14:06:50+0000"
    },
    {
        "title": "PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention",
        "abstract": "Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic complexity of the attention mechanism when processing long contexts. Sparse attention methods offer a promising solution, but existing approaches often suffer from incomplete effective context and/or require complex implementation of pipeline. We present a comprehensive analysis of sparse attention for autoregressive LLMs from the respective of receptive field, recognize the suboptimal nature of existing methods for expanding the receptive field, and introduce PowerAttention, a novel sparse attention design that facilitates effective and complete context extension through the theoretical analysis. PowerAttention achieves exponential receptive field growth in $d$-layer LLMs, allowing each output token to attend to $2^d$ tokens, ensuring completeness and continuity of the receptive field. Experiments demonstrate that PowerAttention outperforms existing static sparse attention methods by $5\\sim 40\\%$, especially on tasks demanding long-range dependencies like Passkey Retrieval and RULER, while maintaining a comparable time complexity to sliding window attention. Efficiency evaluations further highlight PowerAttention's superior speedup in both prefilling and decoding phases compared with dynamic sparse attentions and full attention ($3.0\\times$ faster on 128K context), making it a highly effective and user-friendly solution for processing long sequences in LLMs.\nfor associated code, see https://github.com/w568w/PowerAttention",
        "authors": [
            "Lida Chen",
            "Dong Xu",
            "Chenxin An",
            "Xintao Wang",
            "Yikai Zhang",
            "Jiangjie Chen",
            "Zujie Liang",
            "Feng Wei",
            "Jiaqing Liang",
            "Yanghua Xiao",
            "Wei Wang"
        ],
        "arxiv_categories": [
            "cs.CL",
            "cs.LG"
        ],
        "arxiv_primary_category": "cs.CL",
        "date": "2025-03-05 15:24:11+0000",
        "link": "http://arxiv.org/abs/2503.03588v1",
        "pdf_url": "http://arxiv.org/pdf/2503.03588v1",
        "arxiv_id": "2503.03588v1",
        "processed_at": "2025-03-12 14:05:25+0000",
        "not_found": false,
        "repo": "https://github.com/w568w/PowerAttention",
        "stars": 0,
        "main_task": "Sparse Attention Design for LLMs",
        "contributions": "- **Exponential Receptive Field Growth**: Introduces a sparse attention mechanism allowing each output token to attend to \\(2^d\\) tokens, achieving exponential expansion in the receptive field over layers.\n\n- **Complete Context Extension**: Ensures completeness and continuity in context extension, addressing limitations of existing sparse methods for long-range dependencies.\n\n- **Performance Improvement**: Demonstrates significant performance improvements (5\u201340%) on tasks requiring long-range dependencies compared to static sparse attention methods, while maintaining comparable complexity.",
        "summary": "PowerAttention innovates by exponentially expanding the receptive field in LLMs, ensuring effective context extension and overcoming existing sparse attention limitations. The methodology involves theoretical analysis leading to design improvements that allow each output token to attend to $2^d$ tokens across $d$ layers. Results show significant performance improvements on tasks requiring long-range dependencies and faster processing compared to both static methods and full attention techniques.",
        "analyzed_at": "2025-03-12 14:06:54+0000"
    },
    {
        "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
        "abstract": "An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.\nPublished as a conference paper at ICLR 2025",
        "authors": [
            "Zhixuan Lin",
            "Evgenii Nikishin",
            "Xu Owen He",
            "Aaron Courville"
        ],
        "arxiv_categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "arxiv_primary_category": "cs.LG",
        "date": "2025-03-03 23:35:23+0000",
        "link": "http://arxiv.org/abs/2503.02130v1",
        "pdf_url": "http://arxiv.org/pdf/2503.02130v1",
        "arxiv_id": "2503.02130v1",
        "processed_at": "2025-03-12 14:05:43+0000",
        "not_found": false,
        "repo": "https://github.com/zhixuan-lin/forgetting-transformer",
        "stars": 0,
        "main_task": "The primary task type is \"Language Modeling.\"",
        "contributions": "- Incorporating a forget gate into Transformer attention mechanism for data-dependent score down-weighting.\n- Introduction of Forgetting Attention (FoX) showing superior performance on specific tasks, compatibility with FlashAttention, no positional embeddings required.\n- Development of \"Pro\" block design that enhances the performance of both FoX and Transformers by integrating recurrent model components.",
        "summary": [
            "### Step-by-Step Reasoning:\n\n1. **Identify Technical Innovation**:  \n   - Forgetting gate in Transformers.\n\n2. **Understand Key Methodology**:  \n   - Down-weight unnormalized attention scores data-dependently.\n   - Named as Forgetting Attention and model as FoX.\n\n3. **Highlight Primary Results**:  \n   - Outperforms Transformer on long-context modeling, length extrapolation, short-context tasks.\n   - Matches performance in long-context downstream tasks.\n   - Compatible with FlashAttention; no need for positional embeddings.\n   - Retains superior long-context capabilities over other models.\n\n4. **Additional Innovations**:  \n   - Introduced \"Pro\" block design improving both FoX and Transformer.\n\n#### Summary:\n\nThe paper introduces a Forgetting Attention mechanism by incorporating a forget gate into Transformers, resulting in the Forgetting Transformer (FoX). This method down-weights unnormalized attention scores based on data, enhancing performance across various tasks while maintaining compatibility with FlashAttention. Results show FoX outperforms standard Transformers in several areas and retains superior long-context capabilities over other models. Additionally, a \"Pro\" block design further boosts both FoX's and the Transformer's performance."
        ],
        "analyzed_at": "2025-03-12 14:07:21+0000"
    },
    {
        "title": "A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers",
        "abstract": "Recent theoretical results show transformers cannot express sequential reasoning problems over long input lengths, intuitively because their computational depth is bounded. However, prior work treats the depth as a constant, leaving it unclear to what degree bounded depth may suffice for solving problems over short inputs, or how increasing the transformer's depth affects its expressive power. We address these questions by analyzing the expressive power of transformers whose depth can grow minimally with context length $n$. We show even highly uniform transformers with depth $\\Theta(\\log n)$ can express two important problems: recognizing regular languages, which captures state tracking abilities, and graph connectivity, which underlies multi-step reasoning. Notably, both of these problems cannot be expressed by fixed-depth transformers under standard complexity conjectures, demonstrating the expressivity benefit of growing depth. Moreover, our theory quantitatively predicts how depth must grow with input length to express these problems, showing that depth scaling is more efficient than scaling width or chain-of-thought steps. Empirically, we find our theoretical depth requirements for regular language recognition match the practical depth requirements of transformers remarkably well. Thus, our results clarify precisely how depth affects transformers' reasoning capabilities, providing potential practical insights for designing models that are better at sequential reasoning.\nPreprint",
        "authors": [
            "William Merrill",
            "Ashish Sabharwal"
        ],
        "arxiv_categories": [
            "cs.LG",
            "cs.CC"
        ],
        "arxiv_primary_category": "cs.LG",
        "date": "2025-03-05 23:26:25+0000",
        "link": "http://arxiv.org/abs/2503.03961v1",
        "pdf_url": "http://arxiv.org/pdf/2503.03961v1",
        "arxiv_id": "2503.03961v1",
        "processed_at": "2025-03-12 14:05:58+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Transformer Model Analysis",
        "contributions": "- **Minimal $\\log n$ Depth Expresses Complex Problems:** Transformers with depth growing as $\\Theta(\\log n)$ can recognize regular languages and solve graph connectivity, which fixed-depth transformers cannot under standard complexity conjectures. This showcases the increased expressivity from minimal depth growth.\n  \n- **Depth Scaling Efficiency:** The study quantitatively predicts that increasing transformer depth is more efficient for expanding expressive power than scaling width or using additional reasoning steps.\n\n- **Theoretical vs. Empirical Depth Alignment:** The paper finds empirical evidence supporting theoretical predictions about required depths, especially in recognizing regular languages, aligning practical and theoretical insights on transformer design for sequential reasoning tasks.",
        "summary": "**Summary:**\n\nThe paper demonstrates that transformers with minimal depth growth proportional to $\\log n$ can solve complex problems like recognizing regular languages and determining graph connectivity, which fixed-depth transformers cannot express under standard complexity assumptions. By analyzing the impact of log-depth scaling on expressive power, it shows this approach is more efficient than increasing width or using chain-of-thought steps. Empirical evidence aligns with theoretical depth predictions for practical tasks such as regular language recognition, offering insights into designing models that enhance sequential reasoning capabilities.",
        "analyzed_at": "2025-03-12 14:07:17+0000"
    },
    {
        "title": "A Generic Approach to Quantitative Verification",
        "abstract": "This thesis is concerned with quantitative verification, that is, the verification of quantitative properties of quantitative systems. These systems are found in numerous applications, and their quantitative verification is important, but also rather challenging. In particular, given that most systems found in applications are rather big, compositionality and incrementality of verification methods are essential.   In order to ensure robustness of verification, we replace the Boolean yes-no answers of standard verification with distances. Depending on the application context, many different types of distances are being employed in quantitative verification. Consequently, there is a need for a general theory of system distances which abstracts away from the concrete distances and develops quantitative verification at a level independent of the distance. It is our view that in a theory of quantitative verification, the quantitative aspects should be treated just as much as input to a verification problem as the qualitative aspects are. In this work we develop such a general theory of quantitative verification. We assume as input a distance between traces, or executions, and then employ the theory of games with quantitative objectives to define distances between quantitative systems. Different versions of the quantitative bisimulation game give rise to different types of distances, viz.~bisimulation distance, simulation distance, trace equivalence distance, etc., enabling us to construct a quantitative generalization of van Glabbeek's linear-time--branching-time spectrum. We also extend our general theory of quantitative verification to a theory of quantitative specifications. For this we use modal transition systems, and we develop the quantitative properties of the usual operators for behavioral specification theories.\nHabilitation thesis",
        "authors": [
            "Uli Fahrenberg"
        ],
        "arxiv_categories": [
            "cs.LO"
        ],
        "arxiv_primary_category": "cs.LO",
        "date": "2022-04-24 15:28:21+0000",
        "link": "http://arxiv.org/abs/2204.11302v1",
        "pdf_url": "http://arxiv.org/pdf/2204.11302v1",
        "arxiv_id": "2204.11302v1",
        "processed_at": "2025-03-12 14:05:57+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Task Type: Quantitative Verification",
        "contributions": "- Developed a general theory for system distances abstracted from specific types.\n- Utilized game theory with trace execution distances to define quantitative system distances, enabling multiple distance types (e.g., bisimulation, simulation).\n- Extended the framework to include quantitative specifications via modal transition systems and their properties.",
        "summary": [
            "1. Identify main innovation.\n2. Quantitative verification approach.\n3. Uses distance abstraction.\n4. Abstracts specific distances.\n5. Develops general theory.\n\n6. Outline key methodology.\n7. Employs game theory.\n8. Uses quantitative objectives.\n9. Applies bisimulation games.\n10. Constructs system distance types.\n\n11. Present primary results.\n12. Offers spectrum of verification.\n13. Generalizes van Glabbeek's model.\n14. Extends to specifications.\n15. Utilizes modal transition systems.\n\n####---\nThe paper introduces a novel approach in quantitative verification by abstracting distances between executions, enabling versatile and general application across various types of system analyses. It leverages game theory with quantitative objectives to define new system distance metrics such as bisimulation distance and simulation distance, extending van Glabbeek's linear-time\u2013branching-time spectrum. The methodology culminates in a comprehensive theory that integrates both quantitative and qualitative verification aspects, further applying modal transition systems for specifying behavioral properties."
        ],
        "analyzed_at": "2025-03-12 14:07:21+0000"
    },
    {
        "title": "The best of both worlds: stochastic and adversarial bandits",
        "abstract": "We present a new bandit algorithm, SAO (Stochastic and Adversarial Optimal), whose regret is, essentially, optimal both for adversarial rewards and for stochastic rewards. Specifically, SAO combines the square-root worst-case regret of Exp3 (Auer et al., SIAM J. on Computing 2002) and the (poly)logarithmic regret of UCB1 (Auer et al., Machine Learning 2002) for stochastic rewards. Adversarial rewards and stochastic rewards are the two main settings in the literature on (non-Bayesian) multi-armed bandits. Prior work on multi-armed bandits treats them separately, and does not attempt to jointly optimize for both. Our result falls into a general theme of achieving good worst-case performance while also taking advantage of \"nice\" problem instances, an important issue in the design of algorithms with partially known inputs.",
        "authors": [
            "Sebastien Bubeck",
            "Aleksandrs Slivkins"
        ],
        "arxiv_categories": [
            "cs.LG",
            "cs.DS"
        ],
        "arxiv_primary_category": "cs.LG",
        "date": "2012-02-20 21:29:28+0000",
        "link": "http://arxiv.org/abs/1202.4473v1",
        "pdf_url": "http://arxiv.org/pdf/1202.4473v1",
        "arxiv_id": "1202.4473v1",
        "processed_at": "2025-03-12 14:05:44+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Multi-armed Bandits (Adversarial and Stochastic)",
        "contributions": [
            "1. **Identify the focus**: The paper combines adversarial and stochastic bandit settings.\n2. **Understand algorithm goals**: SAO aims for optimal regret in both settings.\n3. **Note prior work separation**: Previous methods treat these settings separately.\n4. **Recognize unique integration**: SAO integrates both types, optimizing jointly.\n5. **Worst-case performance**: Achieves square-root worst-case regret similar to Exp3.\n6. **Nice instance advantage**: Also achieves logarithmic regret like UCB1 in stochastic rewards.\n\n####---\n- **Integration of Adversarial and Stochastic Bandit Settings**: SAO unifies adversarial and stochastic settings, optimizing for both simultaneously.\n- **Optimal Regret Achievement**: Achieves square-root worst-case regret for adversarial rewards and logarithmic regret for stochastic rewards.\n- **Joint Optimization Approach**: First algorithm to jointly optimize performance in both adversarial and stochastic reward scenarios."
        ],
        "summary": "",
        "analyzed_at": "2025-03-12 14:07:26+0000"
    },
    {
        "title": "A Survey on Data Selection for LLM Instruction Tuning",
        "abstract": "Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.",
        "authors": [
            "Jiahao Wang",
            "Bolin Zhang",
            "Qianlong Du",
            "Jiajun Zhang",
            "Dianhui Chu"
        ],
        "arxiv_categories": [
            "cs.CL"
        ],
        "arxiv_primary_category": "cs.CL",
        "date": "2024-02-04 13:32:01+0000",
        "link": "http://arxiv.org/abs/2402.05123v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05123v1",
        "arxiv_id": "2402.05123v1",
        "processed_at": "2025-03-12 14:05:57+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "Data Selection for LLM Instruction Tuning.",
        "contributions": "These contributions focus on organizing existing knowledge, proposing new frameworks for categorization, and providing comprehensive evaluations that guide future research directions in data selection for instruction tuning.",
        "summary": "The paper introduces a comprehensive survey on selecting high-quality datasets for instruction tuning of large language models (LLMs). It proposes a new taxonomy of data selection methods, evaluates these strategies in detail, and discusses the open challenges and potential future developments in this field.",
        "analyzed_at": "2025-03-12 14:07:11+0000"
    },
    {
        "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
        "abstract": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids re-computing past hidden states during generation, making it faster. As model sizes and context lengths grow, the KV Cache becomes a significant memory bottleneck, which calls for compression methods that limit its size during generation. In this paper, we discover surprising properties of Query (Q) and Key (K) vectors that allow us to efficiently approximate attention scores without computing the attention maps. We propose Q-Filters, a training-free KV Cache compression method that filters out less crucial Key-Value pairs based on a single context-agnostic projection. Contrarily to many alternatives, Q-Filters is compatible with FlashAttention, as it does not require direct access to attention weights. Experimental results in long-context settings demonstrate that Q-Filters is competitive with attention-based compression methods such as SnapKV in retrieval tasks while consistently outperforming efficient compression schemes such as Streaming-LLM in generation setups. Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task with a x32 compression level while reducing the generation perplexity drop by up to 65% in text generation compared to Streaming-LLM.",
        "authors": [
            "Nathan Godey",
            "Alessio Devoto",
            "Yu Zhao",
            "Simone Scardapane",
            "Pasquale Minervini",
            "\u00c9ric de la Clergerie",
            "Beno\u00eet Sagot"
        ],
        "arxiv_categories": [
            "cs.CL",
            "cs.AI"
        ],
        "arxiv_primary_category": "cs.CL",
        "date": "2025-03-04 17:37:49+0000",
        "link": "http://arxiv.org/abs/2503.02812v1",
        "pdf_url": "http://arxiv.org/pdf/2503.02812v1",
        "arxiv_id": "2503.02812v1",
        "processed_at": "2025-03-12 14:06:01+0000",
        "not_found": false,
        "repo": "N/A",
        "stars": 0,
        "main_task": "KV Cache Compression",
        "contributions": "- Proposes Q-Filters, a training-free KV Cache compression method leveraging QK geometry properties to efficiently approximate attention scores without full attention maps.\n- Introduces context-agnostic projection for filtering Key-Value pairs, enhancing compatibility with FlashAttention by avoiding direct access to attention weights.\n- Demonstrates competitive performance in retrieval tasks and superior results in generation setups, achieving 99% accuracy at a x32 compression level with reduced perplexity drop.",
        "summary": "The research introduces Q-Filters, a novel KV Cache compression method that efficiently approximates attention scores without computing attention maps, leveraging properties of Query and Key vectors. The methodology involves a training-free approach using context-agnostic projections to filter out less crucial Key-Value pairs, compatible with FlashAttention. Results show competitive performance in retrieval tasks and superior results in generation setups, achieving 99% accuracy at a high compression level and significantly reducing perplexity drop compared to other methods.",
        "analyzed_at": "2025-03-12 14:07:38+0000"
    }
]